{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c06fe1fb",
   "metadata": {},
   "source": [
    "#Introduction:\n",
    "\n",
    "This document discusses a simple sentence generator program, which mimicks real life language models, as well as details regarding how the algorithm functions. \n",
    "\n",
    "The program functions by recieving a list of sentences, and is able to output new sentences with a given starting word. It'll pick the next word based on random chance, with the probability being determined by how often the word occurs after the previous word in the list of training sentences. \n",
    "\n",
    "If you are too impatient to read the whole document, here are some outputs generated by this program:\n",
    "\n",
    "\"She danced gracefully on the playground.\"\n",
    "\n",
    "\"He fixed the backyard.\"\n",
    "\n",
    "\"I enjoy reading books in the conference.\"\n",
    "\n",
    "\"They went for their exams.\"\n",
    "\n",
    "Please continue reading if you wish to see how the program actually functions, as well as to see its limitations, potential, and some of my other personal thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5a06a9",
   "metadata": {},
   "source": [
    "*Note:*\n",
    "To address the elephant in the room, please keep in mind that this project does not use actual machine learning elements, rather it was merely inspired by the general idea. More information and limitations of the program can be found in the end of the documentation.\n",
    "\n",
    "*Note 2:*\n",
    "Since this is a Jupyter notebook, you should be able to execute these programs directly in this document, provided that you downloaded it and have properly set up Python on your device. However, you need to run the function declarations first before you can run the provided examples. Conversely, there is also the full program included in the same repository for your convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6d36a2",
   "metadata": {},
   "source": [
    "#Section 1: Interpretation\n",
    "\n",
    "This section discusses how the program reads in info, and how it processes said inputs for \"learning\".\n",
    "\n",
    "The first obvious step in decompiling a full sentence is to divide it into individual words, as their individual meanings contribute to the sentence as a whole. The components of a sentence also includes the punctuations, which provide insight on context and division of clauses. \n",
    "\n",
    "Although real language models usually divide the words into even smaller chunks (notably for prefix and suffixes with their own meanings), it is currently beyond this project. In addition, this project currently only considers ending punctuations such as periods.\n",
    "\n",
    "Below is the simple function used to break down a sentence into its significant components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a26f375a-e123-408a-aa1e-75d3d1e2090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function breaks down sentences into digestable chunks for the program to analyze\n",
    "\n",
    "PUNCTS = (\".\", \"?\", \"!\") #Given list of currently accepted punctuations\n",
    "\n",
    "def processSentence(sentence):\n",
    "    chunks = []\n",
    "    nextChunk = \"\"\n",
    "\n",
    "    for i in sentence:\n",
    "        if i in PUNCTS:\n",
    "            chunks.append(nextChunk)\n",
    "            nextChunk = \"\"\n",
    "\n",
    "            chunks.append(i)\n",
    "        elif i == \" \":\n",
    "            if len(nextChunk) > 0:\n",
    "                chunks.append(nextChunk)\n",
    "                nextChunk = \"\"\n",
    "        else:\n",
    "            nextChunk += i\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d704f4",
   "metadata": {},
   "source": [
    "Below is an example of the executed program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f10ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "print(processSentence(sentence))\n",
    "#['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb7efc8",
   "metadata": {},
   "source": [
    "It is very important to note that this program is rather basic, and can only accept simple sentences with a singular independent clause. In other words, punctuations such as commas or semicolons should not be included in a training sentence. Additionally, it is imperative to check for typos or other errors in provided training sentences. More updates and improvements for the program may come in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18852b10",
   "metadata": {},
   "source": [
    "#Section 2: Training\n",
    "\n",
    "This section discusses how the program conform itself to the processed input, in preparation for sentence generating. This is the second level of preprocessing before the program can actually create its own outputs.\n",
    "\n",
    "\n",
    "Now that the program can digest individual sentences, its next goal is to learn from the myriad of provided training sentences given by the user. After breaking a sentence into smaller chunks, the program can analyze how frequently a certain chunk occurs, as well as what words usually come after it. This would be the two main pieces of information utilized by the program in the.\n",
    "\n",
    "For preprocessing, the program needs a list of valid training sentences, the more the better (with more words in common being better also). The preprocessing training section then interprets all of these sentences, while storing the relevant data into a large 2D matrix. This format of matrices is generally known as Markov Matrices, or Markov Chain.\n",
    "\n",
    "The summary of a Markov Matrix is that it represents simple probabilities, where the row or columns each represents the current state and  the next state. The numbers represent the probabilites of the next state is the state corresponding to the current column, given the starting state of the current row. Strictly speaking, the sum of all the numbers in a row should be 1, representing the full range of probabilites. However, this program will slightly bend the rules here, and would deal with that calculations in the generation phase.\n",
    "\n",
    "\n",
    "Below is the code used for creating the Markov Matrix of the given sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de4fb49a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainedMatrix = {\n",
    "    #\"SampleWord\" : [[NextWords], [NextProbs]]\n",
    "}\n",
    "\n",
    "def train(trainingSentences, targetMatrix):\n",
    "    for sen in trainingSentences:\n",
    "        procSen = processSentence(sen)\n",
    "\n",
    "        for i in range(len(procSen)):\n",
    "            chk = procSen[i]\n",
    "            prevChk = procSen[i-1] if i > 0 else None\n",
    "\n",
    "\n",
    "            if not chk in PUNCTS and not chk in targetMatrix.keys():\n",
    "                targetMatrix[chk] = [[], []]\n",
    "            \n",
    "            if prevChk is not None:\n",
    "\n",
    "                if chk in targetMatrix[prevChk][0]:\n",
    "                    updatedWrdList = targetMatrix[prevChk][0]\n",
    "                    updatedProbList = targetMatrix[prevChk][1]\n",
    "\n",
    "                    chkIndex = updatedWrdList.index(chk)\n",
    "                    updatedProbList[chkIndex] += 1\n",
    "\n",
    "                    targetMatrix.update({prevChk : [updatedWrdList, updatedProbList]})\n",
    "                else:\n",
    "                    updatedWrdList = targetMatrix[prevChk][0]\n",
    "                    updatedProbList = targetMatrix[prevChk][1]\n",
    "\n",
    "                    updatedWrdList.append(chk)\n",
    "                    updatedProbList.append(1)\n",
    "\n",
    "                    targetMatrix.update({prevChk : [updatedWrdList, updatedProbList]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd0eab2",
   "metadata": {},
   "source": [
    "Below is an example of the executed program, as well as some notes about the output:\n",
    "\n",
    "The training sentences are provided in a tuple, which you may edit at your own leisure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ee0d47c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': [['b', 'c'], [1, 1]], 'b': [['.', 'c'], [1, 1]], 'c': [['.'], [2]]}\n"
     ]
    }
   ],
   "source": [
    "#Training sentences, each letter represents a word\n",
    "testSentences = (\n",
    "    \"a b.\",\n",
    "    \"b c.\",\n",
    "    \"a c.\"\n",
    ")\n",
    "\n",
    "trainedMatrix = {\n",
    "    #\"SampleWord\" : [[NextWords], [NextProbs]]\n",
    "}\n",
    "\n",
    "train(testSentences, trainedMatrix)\n",
    "\n",
    "print(trainedMatrix)\n",
    "#{'a': [['b', 'c'], [1, 1]], 'b': [['.', 'c'], [1, 1]], 'c': [['.'], [2]]}\n",
    "\n",
    "#Here is a more visual representation of the created Markov Chain:\n",
    "#   a b c .\n",
    "# a 0 1 1 0 (both 'b' and 'c' appeared once after 'a' in all of the sentences)\n",
    "# b 0 0 1 1\n",
    "# c 0 0 0 2 (Note that the period appeared after 'c' twice, hence the 2 here)\n",
    "# . - - - - (ending punctuations are a special case, as the program terminates everytime these symbols were generated)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48dceee8",
   "metadata": {},
   "source": [
    "#Section 3: Generation\n",
    "\n",
    "This section touches on how the program uses all the preprocessed data to actually create its outputs.\n",
    "\n",
    "The fundamental idea of most large problems are to divide it into smaller, more approachable tasks. Language models are no exception to this. As such, before considering how to generate a sentence, it may be helpful to consider how to generate the next word.\n",
    "\n",
    "With the given processed data from the previous sections, we now know what words could be next, as well as their frequencies in the original training data. Therefore, our program can use this to its advantage to choose a random possible word, with the probability being decided by the frequencies of appearance.\n",
    "\n",
    "Imagine a simple number range from 0 to 1. We can divide this range into different, smaller chunks of various sizes (for example: 0.5, 0.3, and 0.2). If we generate a random number between 0 and 1, this number would be in one of these chunks. Larger sized chunks means a higher chance that the random number would fall into its range (In the previous example, 0-0.5 would be the most likely chunk to be chosen). If we take this concept and map each chunk to a corresponding word, we have now created a simple way of picking a random word with varied probabilities.\n",
    "\n",
    "Below are the two functions responsible for this task:\n",
    "\n",
    "*probabilityGradient()* scales a given list of numbers to the confines between 0 and 1.\n",
    "*selectValue()* uses a generated probability gradient and returns the index of a randomly picked chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e298d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "#These functions create scaled chunks based on the given information, and then select a random chunk, see more in the documentation\n",
    "\n",
    "#Helper function, this confines the probability range between 0 and 1\n",
    "def probabilityGradient(probs):\n",
    "    total = sum(probs)\n",
    "    return [i/total for i in probs]\n",
    "\n",
    "#This function returns the index of the randomly chosen section\n",
    "def selectValue(probs):\n",
    "    gradient = probabilityGradient(probs)\n",
    "    target = random.random()\n",
    "    total = 0\n",
    "    result = 0\n",
    "\n",
    "    for bound in gradient:\n",
    "        total += bound\n",
    "\n",
    "        if target <= total:\n",
    "            return result\n",
    "        result += 1\n",
    "\n",
    "    #technically it would never reach this point but just return the last chunk in case it didn't pick anything prior\n",
    "    return len(gradient) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd5a77f",
   "metadata": {},
   "source": [
    "After being able to randomly pick a random chunk, now the program simply assigns the next possible words to each chunk using the trained Markov Matrix from before. Now that it is able to predict the next word, the program can generate a full sentence through recursion, using its previous output as the next input.\n",
    "\n",
    "In the case that the previous word has never be encountered before (if it was not part of the training data), the program would simply halt itself. More information are in the documentation below regarding domains and limitations. Additionally, to prevent potential infinite rescursions, a stopgap solution of a 50 words limit is set here.\n",
    "\n",
    "Below is the main function for generating a sentence, which contains a rescursive sub function for generating each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7ed1d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENT_LIM = 50 #Hard stop maximum sentence length to prevent possible infinite recursion\n",
    "\n",
    "def generate(startWrd, matrix):\n",
    "    sent = [startWrd]\n",
    "\n",
    "    if not startWrd in matrix.keys():\n",
    "        print(\"Starting word hasn't been learnt before\")\n",
    "        return None\n",
    "\n",
    "    def generateRec(prevWrd):\n",
    "        if len(sent) >= MAX_SENT_LIM:\n",
    "            return False\n",
    "        else:\n",
    "            nxtWords = matrix[prevWrd][0]\n",
    "            nextProbs = matrix[prevWrd][1]\n",
    "\n",
    "            nxtWord = nxtWords[selectValue(nextProbs)]\n",
    "            sent.append(nxtWord)\n",
    "\n",
    "            if nxtWord in PUNCTS:\n",
    "                return True\n",
    "            else:\n",
    "                return generateRec(nxtWord)\n",
    "\n",
    "    outcome = generateRec(startWrd)\n",
    "\n",
    "    if outcome:\n",
    "        print(\"Success, returning current result\")\n",
    "        return(sent)\n",
    "    else:\n",
    "        print(\"Exceeded maximum word limit, returning current result\")\n",
    "        return(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e58754",
   "metadata": {},
   "source": [
    "#Section 4: Results\n",
    "\n",
    "This section provides example input and outputs of the program.\n",
    "\n",
    "The training sentences are provided in a tuple, which you may edit at your own leisure. However, keep in mind the aforementioned restrictions and domains of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c522ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success, returning current result\n",
      "['They', 'built', 'a', 'snowman', 'in', 'my', 'free', 'time', '.']\n"
     ]
    }
   ],
   "source": [
    "MAX_SENT_LIM = 50 #Hard stop maximum sentence length to prevent possible infinite recursion\n",
    "PUNCTS = (\".\", \"?\", \"!\") #Valid ending punctuations\n",
    "\n",
    "TRAIN_SENT = ( #Example sentences generously provided by ChatGPT 3.5\n",
    "    \"The cat is sleeping on the mat.\",\n",
    "    \"I enjoy reading books in my free time.\",\n",
    "    \"She greeted me with a warm smile.\",\n",
    "    \"The students are studying for their exams.\",\n",
    "    \"They went for a walk in the park.\",\n",
    "    \"The flowers in the garden are blooming beautifully.\",\n",
    "    \"She is wearing a red dress to the party.\",\n",
    "    \"They built a sandcastle on the beach.\",\n",
    "    \"We visited the museum to see the art exhibition.\",\n",
    "    \"The baby giggled at the funny sounds.\",\n",
    "    \"He fixed the broken chair with a hammer.\",\n",
    "    \"The birds are chirping in the trees.\",\n",
    "    \"She danced gracefully on the stage.\",\n",
    "    \"He painted a beautiful landscape on the canvas.\",\n",
    "    \"The rain is pouring heavily outside.\",\n",
    "    \"She gave a speech at the conference.\",\n",
    "    \"He won a gold medal in the swimming competition.\",\n",
    "    \"The children played happily in the playground.\",\n",
    "    \"They built a snowman in the backyard.\",\n",
    "    \"We took a family photo during the vacation.\",\n",
    "    \"I solved the puzzle in record time.\",\n",
    "    \"They went on a road trip across the country.\",\n",
    "    \"We attended a music concert in the city.\",\n",
    "    \"I practiced playing the piano every day.\",\n",
    "    \"They enjoyed a sunset cruise on the lake.\",\n",
    "    \"We volunteered at a local charity event.\"\n",
    ")\n",
    "\n",
    "#Actual code, run all of the function declarations in the previous function for this part to work.\n",
    "train(TRAIN_SENT, trainedMatrix)\n",
    "\n",
    "#Select a random valid starting word, taken from the training sentences\n",
    "START_WORDS = (\"I\", \"He\", \"She\", \"They\", \"The\")\n",
    "startWrd = START_WORDS[int(random.random() * len(START_WORDS))]\n",
    "\n",
    "#This will provide a random result when you run it every time, have fun\n",
    "print(generate(startWrd, trainedMatrix))\n",
    "\n",
    "#The result is a list of the words, the official version of the program provides a helper function that turns it back into one string, but it is not included here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6209e69",
   "metadata": {},
   "source": [
    "#Section 5: Implications and Limitations\n",
    "\n",
    "This section discusses more about the general idea as well as possible future features that can greatly improve this very basic program.\n",
    "\n",
    "###1: Domain of Input Sentences\n",
    "\n",
    "Currently, this program only accept very simple sentences, with only one independent clause. There is also a small given list of accepted ending punctuations (period, question mark, exclamation mark), and no commas are currently accepted as part of a valid training sentence. Despite this, the basis of multi-clause sentences are already avaliable, and may be updated in the near future.\n",
    "\n",
    "###2: Sentence \"Comprehension\"\n",
    "\n",
    "It is obvious that this program generates rather nonsensical outputs, due to the recursive method only considering the previous word. Larger, more advanced language models takes into account much more information. However, assuming that the provided inputs are valid, the program's inherent considerations of word frequency and order would still create broad patterns. As such, the resultant sentences still resemble human speech, and are far from pure randomness.\n",
    "\n",
    "###3: Word Choice\n",
    "\n",
    "Due to resource and personal limitations, this program simply picks words based on pure randomness, and the probability is only affected by appearance frequencies. In addition, if the training sentences are too unique from one another (sharing little words in common), there would be less opportunities for the program to creatively branch off. As such it'll just create one of the original training sentences, which is unfortunate and boring.\n",
    "\n",
    "However theoretically, it may be possible to adjust and skew the probabilities using concepts from linear algebra. For example, a word can be weighted more heavily from another word to favor the former in generation. This can be achieved through matrix multiplications, and could be incredibly useful to control the feel of the result, such as making it sound more positive or negative in general. Such a idea may be useful for machine learning field, where there generally lacks the ability to fine tune a program's output. \n",
    "\n",
    "Unfortunately, more development into biases and weights would be needed for the generation of a more nuanced and clear narrative, and I'm not sure if I have the time or knowledge to pull it off."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba716a5",
   "metadata": {},
   "source": [
    "#Conclusion:\n",
    "\n",
    "Overall, although this program obviously have little practical purposes (due to its limitations as well as being unable to compete with real language models), it may act as an oversimplified example for curious beginners who are looking into the field of artificial intelligence. At least, I had a fun time making this, and I hope that this document has been entertaining to read as well as being somewhat educational and insightful.\n",
    "\n",
    "Thank you for reading."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
